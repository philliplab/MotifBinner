```{r Setup, include=FALSE}
library(MotifBinner)
```

# Investigation of various approach to treating mislabel detection

## Overview

Given a bin of sequences that are theoretically from only a single virus
molecule, remove all of those that are actually from a different molecule.

This is accomplished by looking at the distances between each of the sequences
and removing the outliers.

The difficult parts are deciding how to compute the distances efficiently and
setting the thresholds that start and stop the process.

This process is further complicated by these concerns:
1) bin sizes varies from 1 to several hundred
2) We have to deal with indels
3) This process must be completely automated
4) This process must be very computationally efficient

## Testing strategy

We need to test the system using bins with known answers. 

### Test Data

As always, the design data structure for the test data is important. Use a
structure like this:

```{r eval=FALSE, echo=TRUE}
list('test1' = list('in' = DNAStringSet(...),
                    'out' = DNAStringSet(...)),
     'test2' = ...)
```

The data is available in the package:
```{r echo=TRUE}
test_dat <- get_mislabel_test_data()
```

Using this data for a 'bin' by putting the 'src' and 'out' data together and check
that only the 'out' data is removed and all the 'src' data is kept.

### Basic Code for running tests

Basic code to run the tests is available from the package:

```{r, eval=FALSE, echo=TRUE}
score_classification()
```

### Metrics of interest

A number of metrics must be considered when looking at the accuracy of
classification.

Keep it basic. 

Consider these standard classification metrics:

#### Sensitivity:
Number of true 'in' classifications / (Total size of true 'in' population)

#### Specificity:
Number of true 'out' classifications / (Total size of the 'out' population)

Maximize both simultaneously

Now, in addition to these two, there are other metrics of interest that we can
derive based on our knowledge of the system.

#### Maximum distance in the final dataset: 
We know that the only source of errors
should be the sequencing process. We have access to data about the error rates
of the sequencing process. We can use this to make a statement like:

The sequencing process is accurate to such a degree that no two reads of the
same molecule should differ by more than one base per 100 bases. Build a metric
around this information.

#### Speed

The time it took to classify the reads in the bin.

#### Last words about the metrics

The euclidean distance from the sensitivity and specificity from (1, 1) will be
reports as 'combo' for each test. If the average of this column for all test
datasets is 0, then we have a perfect classifier. This is the metric of
interest to be on the lookout for.

## The different strategies

Design and implement a who set of strategies and then benchmark them to find
the best ones.

### Remove None

This strategy keeps all the data

Use the random strategy but set the parameter 'n' to 0. So that 0 percent of
the data will be randomly removed.

```{r}
kable(score_all_classifications(test_dat, 'random', params = list(n=0)), digits = 2)
```

### Remove Random

This strategy removes 40% of the sample at random

```{r}
kable(score_all_classifications(test_dat, 'random', params = list(n=0.4)), digits = 2)
```
### Information Variance Balance

This strategy will keep on removing the most outlying sequence as long as is
leads to a percentage reduction in variance that is x times larger than the
percentage of information that was discarded

#### Threshold of 1
```{r}
kable(score_all_classifications(test_dat, 'infovar_balance', params = list(threshold = 1)), digits = 2)
```

#### Threshold of 2
```{r}
kable(score_all_classifications(test_dat, 'infovar_balance', params = list(threshold = 2)), digits = 2)
```

#### Threshold of 3
```{r}
kable(score_all_classifications(test_dat, 'infovar_balance', params = list(threshold = 3)), digits = 2)
```

### Most Frequent Sequence
```{r}
kable(score_all_classifications(test_dat, 'most_frequent', params = list()), digits = 2)
```

### The silver bullet

To be devised

## Meeting Agenda

- Simulation of test data for MotifBinner
  - grinder
  - GemSim
  - Evolveagene
  - poorly understood and supremely complex MiSeq error profiles
  - Why am I trying to duplicate the entire process, I should consider a more focussed simulation approach (Available software forces me to, but I should rather adapt polyester?)
- Testing strategies for MotifBinner
  - Benchmarking system demonstration
  - Why am I benchmarking the mislabel detection? Should I just benchmark the entire process?
- Mislabel detection strategies for MotifBinner
  - Why kinky diagrams failed
  - Distance computation inefficiencies caused by duplicates and possible hacks
  - Describe the information variance balance strategy
  - How relevant are the metrics Im using for mislabel detection? Since
    technically we only care about the consensus sequence. Is all that
    matters that none of the 'out' sequences makes it into the 'src' bin?
  - The most "frequent" strategy and some brainstorming about what can go wrong
    - Two+ variant versions
- Gaps and consensus sequences and homopolymer errors
